{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:68f863d97e1693cac7d16f08c042d7228dcf916d46dae77454d8cecfff331ac1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Model Selection\n",
      "\n",
      "In this programming assignment we examine techniques for model selection on classification and regression tasks. In particular, we first explore the effect of model hyperparameters on the bias and variance of the prediction. In the second part of the assignment we utilize the bias-variance decomposition to perform automatic hyperparameter selection. Several classes and methods are provided in the `utils.py` file:\n",
      "\n",
      "### Datasets\n",
      "\n",
      "* **`utils.Housing()`:** This regression dataset is available at http://archive.ics.uci.edu/ml/datasets/Housing and loaded from scikit-learn's inbuilt representation. This data is used for regression. A description of the dataset can be found here http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names. This data is in a `506x13` matrix and the labels in a array of length `506`.\n",
      "\n",
      "* **`utils.Yeast()`:** This classification dataset is available at https://archive.ics.uci.edu/ml/datasets/Yeast . This data is used for classification. A description of the dataset can be found here https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.names. This data is in a `1484x8` matrix and the labels (class probabilities) are in a `1484x7` matrix where `targets[i,j] = 1` if example `i` is of class `j` and `0` otherwise. For example, if we have a dataset of 4 examples which belong to following classes : `[1, 0, 0, 2]` the label matrix would look like this: `T = [[0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1]]`.\n",
      "\n",
      "### Predictors\n",
      "\n",
      "We provide two simple classes of predictors, one for regression and one for classification:\n",
      "\n",
      "* **`utils.ParzenRegression`:** A regression method based on Parzen window. The hyperparameter corresponds to the scale of the Parzen window. A large scale creates a more rigid model. A small scale creates a more flexible one.\n",
      "\n",
      "* **`utils.ParzenClassification`:** A classification method based on Parzen window. The hyperparameter corresponds to the scale of the Parzen window. A large scale creates a more rigid model. A small scale creates a more flexible one. Note that instead of returning a single class for a given data point, it outputs a probability distribution over the set of possible classes.\n",
      "\n",
      "Each class of predictor implements the following three methods:\n",
      "\n",
      "  - **`__init__(self,parameter):`** Create an instance of the predictor with a certain scale parameter.\n",
      "\n",
      "  - **`fit(self,X,T):`** Fit the predictor to the data (a set of data points `X` and targets `T`).\n",
      "    \n",
      "  - **`predict(self,X):`** Compute the output values arbitrary inputs `X`.\n",
      "  \n",
      "### Bias Variance Decomposition\n",
      "\n",
      "As we have seen in the theoretical exercise, there are several possible bias-variance decomposition for different tasks (e.g. classification, or regression).\n",
      "\n",
      "* **`utils.biasVarianceRegression():`** Perform the usual bias-variance decomposition of the mean square error. Reminder: given $Y$ the (random) estimator and $T$ the target, the decomposition is computed as follows:\n",
      "\n",
      "  - $\\mathrm{Bias}(Y)^2 = (\\mathbb{E}_Y [ Y - T ])^2$\n",
      "\n",
      "  - $\\mathrm{Var}(Y)$ $= \\mathbb{E}_Y [(Y - \\mathbb{E}_Y[Y])^2 ]$\n",
      "\n",
      "  - $\\mathrm{Error}(Y)$ = $\\mathbb{E}_Y[(Y-T)^2]$\n",
      "\n",
      "### Sampler\n",
      "\n",
      "To compute the bias and variance estimates, we require *multiple samples* from the training set for a single set of observation data. To acomplish this, we utilize the **`Sampler`** class provided. The sampler is initialized with the training data and passed to the method for estimating bias and variance, where its function **`sampler.sample()`** is called repeatedly in order to fit multiple models and create an ensemble of prediction for each test data point."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 1: Implementing Bias-Variance Decomposition for Classification (20 P)\n",
      "\n",
      "Implement a function which computes the bias, variance and error given the true labels of the training data and the predicted values. Bias, Variance and Error for classification are defined as:\n",
      "\n",
      "- $\\mathrm{Bias}(Y) = D_\\mathrm{KL}(T||R)$\n",
      "\n",
      "- $\\mathrm{Var}(Y) = \\mathbb{E}_Y[D_\\mathrm{KL}(R||Y)]$\n",
      "\n",
      "- $\\mathrm{Error}(Y) = \\mathbb{E}_Y[D_\\mathrm{KL}(T||Y)]$\n",
      "\n",
      "where $R$ is the distribution that minimizes its expected KL divergence from the estimator of probability distribution $Y$ (see the theoretical exercise for how it is computed exactly), and where $T$ is the target class distribution. Note that we consider here the Kullback-Leibler divergence as a measure of classification error, which is commonly done in practice in order to have a smooth objective function.\n",
      "\n",
      "**Tasks:**\n",
      "\n",
      "* **Implement the KL-based Bias-Variance Decomposition defined above (10 P)**\n",
      "\n",
      "  To get started, you can take inspiration from the readily implemented function `utils.biasVarianceRegression()`, which does the following:\n",
      "\n",
      "  - Iterate for a certain number of times the following:\n",
      "\n",
      "    - Acquire a subsample of the training data by invoking `sampler.sample()`\n",
      "    \n",
      "    - Using the predictor (which will either be a Parzen Regressor or Parzen Classifier depending on the task), fit the model on the sample and determine the prediction for the observation data ($N$ examples disjoint from the training data). Note that the dimension of the outputs matches the dimension of the targets, so for regression you will get an array of length $N$ and for classification a matrix of shape $N \\times \\#\\text{classes}$ containing the class distributions.\n",
      "\n",
      "  - Having computed a number of different predictions, determine the bias, variance and error comparing the predictions to the true labels. Check that the decomposition is correct (i.e. bias + variance = error) using an **`assert`** statement, and return the bias and variance.\n",
      "\n",
      "\n",
      "* **Once the method is implemented, run Test 1 and Test 2 provided below (10 P)**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def biasVarianceClassification(sampler, predictor, X, T, nbsamples=25):\n",
      "    # --------------------------------\n",
      "    # TODO: REPLACE BY YOUR CODE\n",
      "    # --------------------------------\n",
      "    import solutions\n",
      "    return solutions.biasVarianceClassification(sampler, predictor, X, T, nbsamples=nbsamples)\n",
      "    # --------------------------------"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### TEST 1\n",
      "import utils,numpy\n",
      "%matplotlib inline\n",
      "utils.plotBVE(utils.Housing,numpy.logspace(-6,3,num=30),utils.ParzenRegressor,utils.biasVarianceRegression,'YourGroupName: Housing Regression')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### TEST 2\n",
      "import utils,numpy\n",
      "%matplotlib inline\n",
      "utils.plotBVE(utils.Yeast,numpy.logspace(-6,3,num=30),utils.ParzenClassifier,biasVarianceClassification,'YourGroupName: Yeast Classification')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Part 2: Implementing a Parameter Selection Procedure (30 P)\n",
      "\n",
      "In this part of the exercise, we would like to find what is the best hyperparameter of the model for predicting the Housing regression data. A 5-fold cross-validation procedure is already implemented and that allows to compute error bars.\n",
      "\n",
      "![Loop](files/loop.png)\n",
      "\n",
      "You need to extend this basic cross-validation procedure by a nested loop of 4-fold cross-validation that selects the best hyperparameters based on some criterion (cost function) to be determined. The nested loop is depicted below:\n",
      "\n",
      "![Nested](files/nested.png)\n",
      "\n",
      "The full procedure for evaluation and hyperparameter selection procedure is shown in the diagram below with the part that you need to implement highlighted in red.\n",
      "\n",
      "![Procedure](files/procedure.png)\n",
      "\n",
      "**Tasks:**\n",
      "\n",
      "\n",
      "- **Implement the inner loop of 4-fold cross-validation, helping you from the diagram above (20 P)**\n",
      "\n",
      "  For this part, use the following settings:\n",
      "\n",
      "  - Range of parameters to test: 15 parameters logarithmically spaced between 1e-5 and 1e5.\n",
      "  - The returned parameter is the geometric mean of the best parameter found for each split.\n",
      "  - The best parameter for each split is the one that minimizes the `costfunction` specified as argument.\n",
      "  - The bias and variance estimates are obtained by sampling 10 times from the training distribution.\n",
      "\n",
      "\n",
      "- **Verify your implementation by running Test 3 (10 P)**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy,utils\n",
      "\n",
      "def getbestparameter(Xselect,Tselect,costfunction):\n",
      "    # --------------------------------\n",
      "    # TODO: REPLACE BY YOUR CODE\n",
      "    # --------------------------------\n",
      "    import solutions\n",
      "    return solutions.getbestparameter(Xselect,Tselect,costfunction)\n",
      "    # --------------------------------"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy,utils\n",
      "\n",
      "def evaluateModel(X,T,costfunction): \n",
      "    # X: partitioned input\n",
      "    # T: partitioned targets\n",
      "    # costfunction: the function for evaluate how good/bad a hyperparameter is\n",
      "    \n",
      "    # Create splits\n",
      "    splits = [ ([1,2,3,4],0) , ([0,2,3,4],1) , ([0,1,3,4],2) , ([0,1,2,4],3) , ([0,1,2,3],4) ]\n",
      "    \n",
      "    testbiases,testvariances,testerrors,bestparameters = [],[],[],[]\n",
      "\n",
      "    #Loop over selection/test splits \n",
      "    for inds_select,ind_test in splits: \n",
      "         \n",
      "        Xselect = [X[ind] for ind in inds_select]\n",
      "        Tselect = [T[ind] for ind in inds_select]\n",
      "\n",
      "        Xtest = X[ind_test]\n",
      "        Ttest = T[ind_test]\n",
      "        \n",
      "        bestparam = getbestparameter(Xselect,Tselect,costfunction)\n",
      " \n",
      "        # Evaluate bias and variance with this best parameter\n",
      "        predictor = utils.ParzenRegressor(bestparam)\n",
      "        sampler   = utils.Sampler(numpy.concatenate(Xselect,axis=0),numpy.concatenate(Tselect,axis=0))\n",
      "        bias,variance = utils.biasVarianceRegression(sampler,predictor,Xtest,Ttest, nbsamples=20)\n",
      "        \n",
      "        testbiases     += [bias]\n",
      "        testvariances  += [variance]\n",
      "        testerrors     += [bias+variance]\n",
      "        bestparameters += [bestparam]\n",
      " \n",
      "    # Output results of model evaluation\n",
      "    print('bias:      %8.5f +/- %8.5f'%(numpy.mean(testbiases),numpy.std(testbiases))) \n",
      "    print('variance:  %8.5f +/- %8.5f'%(numpy.mean(testvariances),numpy.std(testvariances))) \n",
      "    print('error:     %8.5f +/- %8.5f'%(numpy.mean(testerrors),numpy.std(testerrors)))\n",
      "    print('parameter: %8.5f +/- %8.5f'%(numpy.mean(bestparameters),numpy.std(bestparameters)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### TEST 3\n",
      "\n",
      "import numpy,utils\n",
      "\n",
      "costfunctions = [\n",
      "         ('Parameter Selection Criterion: favor low bias', lambda b,v: 9*b+v),\n",
      "         ('Parameter Selection Criterion: favor low error',lambda b,v: b+v),\n",
      "         ('Parameter Selection Criterion: favor low variance',lambda b,v: b+9*v),\n",
      "]\n",
      "\n",
      "# Load and partition the data\n",
      "X,T = utils.Housing()\n",
      "n = len(X)\n",
      "X = [X[n*i//5:n*(i+1)//5] for i in range(5)]\n",
      "T = [T[n*i//5:n*(i+1)//5] for i in range(5)]\n",
      "\n",
      "print \"YourGroupName\"\n",
      "for name,costfunction in costfunctions:\n",
      "    print('\\n\\n%s\\n'%name)\n",
      "    evaluateModel(X,T,costfunction)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}